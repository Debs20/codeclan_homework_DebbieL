---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(janitor)
library(dplyr)
library(cluster)
library(factoextra)
library(dendextend)
```



```{r}
computers <- read_csv("data/computers.csv")
```

```{r}
#This data does look potentially good for clustering but will need more investigation
head(computers)
```

```{r}
dim(computers)
```

```{r}
class(computers)
```

```{r}
any(is.na(computers))
```

```{r}
#standardising the data

computers1 <- computers %>%
  clean_names()
```

```{r}
computers1 <- computers %>%
    select(hd, ram)

head(computers1)
```

```{r}
ggplot(computers1, aes(hd, ram)) +
  geom_point()
```


```{r}
summary(computers1)
```
```{r}

computers1_scale <- computers1 %>%
                    mutate_all(scale)
 
computers1_scale
```

```{r}
library(broom)

clustered_computers <- kmeans(computers1_scale, centers = 6, nstart = 25)
```


```{r}
tidy(clustered_computers, 
     col.names = colnames(computers1_scale))
```

```{r}
glance(clustered_computers)

```

```{r}
augment(clustered_computers, computers1)
```

```{r}
library(animation)

computers1_scale %>% 
  kmeans.ani(centers = 6) 
```


```{r}
glance(clustered_computers)
```

```{r}
max_k <- 20 

k_clusters <- tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~ kmeans(computers1_scale, .x, nstart = 25)), 
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, computers1)
  )

k_clusters
```

```{r}
library(factoextra)
```

```{r}
fviz_nbclust(computers1_scale, kmeans, method = "wss", nstart = 25)
```

```{r}
fviz_nbclust(computers1_scale, kmeans, method = "silhouette", nstart = 25)
```


```{r}
fviz_nbclust(computers1_scale, kmeans, method = "gap_stat", nstart = 25, k.max = 10)
```


```{r}
 clusterings %>% 
  unnest(cols = c(augmented)) %>%
  filter(k <= 6) %>%
 ggplot(aes(x = hd, y = ram)) +
  geom_point(aes(color = .cluster)) + 
  facet_wrap(~ k)
```


```{r}
clusterings %>% 
  unnest(augmented) %>%
  filter(k == 2) %>%
  group_by(.cluster) %>%
  summarise(mean(hd), mean(ram))
```



CODECLAN SOLUTION

```{r}
library(janitor)
library(fastDummies)
library(broom)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(factoextra)
```

```{r}
pc_sales <- read_csv("data/computers.csv")
```


```{r}
head(pc_sales)
```

Select hd and ram variables and visualise

```{r}
pc_sales <- pc_sales %>%
  select(c(hd, ram))

ggplot(pc_sales, aes(hd, ram)) +
  geom_point()
```

There is some dispersion in the data but perhaps not very clear circular groups of points (i.e. heterogeneous clusters). But let’s give it a go to see what we get!

Scale and check.

```{r}
pc_sales_scale <- pc_sales %>% 
                      mutate_all(scale)

summary(pc_sales_scale)
```

Check each of the methods of choosing k:

```{r}
library(factoextra)
```

```{r}
fviz_nbclust(pc_sales_scale, kmeans, method = "wss", nstart = 25)
```

This graph is fairly smooth curve - rather than a defined kink. Some arguement that kink at k = 2 or k = 4.

```{r}
fviz_nbclust(pc_sales_scale, kmeans, method = "silhouette", nstart = 25)
```

Silhouette method is giving k = 10 but the values for k = 2,7,9 are very close to the silhouette width, so very close margin that k=10 has been picked as optimal.

```{r}
fviz_nbclust(pc_sales_scale, kmeans, method = "gap_stat") #would put nstart=25 if had more computing power
```

This gives a result of k = 3.

We get quite different results for each of the methods and for elbow and silhouette many values of k give quite similar results. This can be a sign data is not well suited for k-means clustering.

From the elbow method and the silhouette coefficient going to go for k=2.

```{r}
clustered_pc_sales <- kmeans(pc_sales_scale, 2, nstart = 25)

clustered_pc_sales
```

Pull out the cluster means and sizes for your chosen number of clusters.

```{r}
clustered_pc_sales$size
```

```{r}
clustered_pc_sales$centers
```

Visualise the clusters.

```{r}
clusters <- augment(clustered_pc_sales, pc_sales)

ggplot(clusters, aes(x = hd, y = ram, colour = .cluster)) +
  geom_point() 
```

Comment on the results:

Judging from the visualisation the points within cluster 1 are very different - RAM ranges from 4 to 32 and HD ranges from 340 to 2,100 (both almost as wide as the full spread of the dataset). So it doesn’t seem like the computers within cluster 1 are particularly similar in attributes e.g. say we were using clustering to find groups of similar computers to create a new product for - the product would need to cater for pretty much any of the computer in the dataset rather than target a subgroup which had similar ram/hd levels.

Would potentially try other values of k or may look into a different type of clustering for this data.