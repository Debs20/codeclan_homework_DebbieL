---
title: "R Notebook"
output: html_notebook
---

```{r}
install.packages("HH")
```

```{r}
install.packages("bestNormalize")
```

```{r}
library(HH)
library(GGally)
library(bestNormalize)
library(dplyr)
library(janitor)
library(leaps)
library(tidyverse)
library(dplyr)
library(modelr)
```

```{r}
install.packages("glmulti")
```

```{r}
library(glmulti)
```

```{r}
avacado <- read.csv("data/avocado.csv") %>% clean_names()
```

```{r}
head(avacado)
```


```{r}
avacado2 <- avacado %>% 
    dplyr::select(-c("date", "region", "x"))
```

```{r}
head(avacado2)
```

```{r}
regsubsets_forward <- regsubsets(average_price ~ ., data = avacado2, nvmax = 10, method = "forward")
```


```{r}
sum_regsubsets_forward <- summary(regsubsets_forward)
sum_regsubsets_forward
```

The best predictor model shows us the best predictors using the asterices

```{r}
# plotting this shows us the adjusted r2 values and which variables are in the model. Top row shows model with highest adjusted r2
plot(regsubsets_forward, scale = "adjr2")
```


```{r}
sum_regsubsets_forward$which
```

```{r}
regsubsets_backward <- regsubsets(average_price ~ ., data = avacado2, nvmax = 10, method = "backward")
```

```{r}

# plotting this shows us the adjusted r2 values and which variables are in the model. Top row shows model with highest adjusted r2
plot(regsubsets_backward, scale = "adjr2")
```


```{r}
regsubsets_exhaustive <- regsubsets(average_price ~ ., data = avacado2, nvmax = 10, method = "exhaustive")
```

```{r}

# plotting this shows us the adjusted r2 values and which variables are in the model. Top row shows model with highest adjusted r2
plot(regsubsets_exhaustive, scale = "adjr2")
```


```{r}
summary(regsubsets_exhaustive)$which[10,]
```


```{r}
summary(regsubsets_backward)$which[10,]
```

```{r}
summary(regsubsets_forward)$which[10,]
```


```{r}
avacado2 %>%
  ggplot(aes(x = average_price)) +
  geom_histogram()
```

```{r}
avacado2 %>%
  ggplot(aes(x = log10(average_price))) +
  geom_histogram()
```


CODECLAN- SOLUTION

```{r}
avocados <- clean_names(read_csv("data/avocado.csv"))
```

```{r}

summary(avocados)

```

```{r}
head(avocados)
```

```{r}
avocados %>%
  distinct(region) %>%
  summarise(number_of_regions = n())
```

```{r}
avocados %>%
  distinct(date) %>%
  summarise(
    number_of_dates = n(),
    min_date = min(date),
    max_date = max(date)
  )

```

```{r}
library(lubridate)
```

```{r}
trimmed_avocados <- avocados %>%
  mutate(
    quarter = as_factor(quarter(date)),
    year = as_factor(year),
    type = as_factor(type)
  ) %>%
  dplyr::select(-c("x1", "date"))
```

```{r}
alias(average_price ~ ., data = trimmed_avocados )
```

```{r}
trimmed_avocados %>%
  dplyr::select(-region) %>%
  ggpairs()
```


```{r}
ggsave("pairs_plot_choice1.png", width = 10, height = 10, units = "in")
```


```{r}
trimmed_avocados %>%
  ggplot(aes(x = region, y = average_price)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

Test competing models with x4046, type, year, quarter and region:

```{r}
model1a <- lm(average_price ~ x4046, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model1a)
```

```{r}
summary(model1a)
```

```{r}
model1b <- lm(average_price ~ type, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model1b)
```

```{r}
summary(model1b)
```


```{r}
model1c <- lm(average_price ~ year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model1c)
```

```{r}
summary(model1c)
```

```{r}
model1d <- lm(average_price ~ quarter, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model1d)
```

```{r}
summary(model1d)
```

```{r}
model1e <- lm(average_price ~ region, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model1e)

```

```{r}
summary(model1e)
```

model1b with type is best, so we’ll keep that and re-run ggpairs() with the residuals (again omitting region).


```{r}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model1b) %>%
  dplyr::select(-c("average_price", "type", "region"))

ggpairs(avocados_remaining_resid)
```

```{r}
ggsave("pairs_plot_choice2.png", width = 10, height = 10, units = "in")
```


```{r}
trimmed_avocados %>%
  add_residuals(model1b) %>%
  ggplot(aes(x = region, y = resid)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

Looks like x4046, year, quarter and region are our next strong contenders:

```{r}
model2a <- lm(average_price ~ type + x4046, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model2a)
```

```{r}
summary(model2a)
```

```{r}
model2b <- lm(average_price ~ type + year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model2b)
```

```{r}
summary(model2b)
```

```{r}
model2c <- lm(average_price ~ type + quarter, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model2c)
```


```{r}
summary(model2c)

```

```{r}
model2d <- lm(average_price ~ type + region, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model2d)
```

```{r}
summary(model2d)
```

So model2d with type and region comes out as better here. We have some region coefficients that are not significant at 0.05 level, so let’s run an anova() to test whether to include region

```{r}
anova(model1b, model2d)
```

It seems region is significant overall, so we’ll keep it in!

```{r}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model2d) %>%
  dplyr::select(-c("average_price", "type", "region"))

ggpairs(avocados_remaining_resid)
```


```{r}
ggsave("pairs_plot_choice3.png", width = 10, height = 10, units = "in")
```


```{r}
model3a <- lm(average_price ~ type + region + x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model3a)
```


```{r}
summary(model3a)
```


```{r}
model3b <- lm(average_price ~ type + region + year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model3b)
```


```{r}
summary(model3b)
```



```{r}
model3c <- lm(average_price ~ type + region + quarter, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model3c)
```

```{r}
summary(model3c)
```

So model3c with type, region and quarter wins out here. Everything still looks reasonable with the diagnostics, perhaps some mild heteroscedasticity.

```{r}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model3c) %>%
  dplyr::select(-c("average_price", "type", "region", "quarter"))

ggpairs(avocados_remaining_resid)
```


```{r}
ggsave("pairs_plot_choice4.png", width = 10, height = 10, units = "in")

```

```{r}
model4a <- lm(average_price ~ type + region + quarter + x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model4a)
```

```{r}
summary(model4a)
```

```{r}
model4b <- lm(average_price ~ type + region + quarter + year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model4b)
```

```{r}
summary(model4b)
```

Hmm, model4b with type, region, quarter and year wins here

```{r}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model4b) %>%
  dplyr::select(-c("average_price", "type", "region", "quarter", "year"))

ggpairs(avocados_remaining_resid)
```

```{r}
ggsave("pairs_plot_choice5.png", width = 10, height = 10, units = "in")
```

It looks like x_large_bags is the remaining contender, let’s check it out!

```{r}
model5 <- lm(average_price ~ type + region + quarter + year + x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5)
```

```{r}
summary(model5)
```

It is a significant explanatory variable, so let’s keep it. Overall, we still have some heterscedasticity and deviations from normality in the residuals.


Let’s now think about possible pair interactions: for five main effect variables we have ten possible pair interactions. Let’s test them out.

```{r}
model5pa <- lm(average_price ~ type + region + quarter + year + x_large_bags + type:region, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pa)
```

```{r}
summary(model5pa)
```


```{r}
model5pb <- lm(average_price ~ type + region + quarter + year + x_large_bags + type:quarter, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pb)
```

```{r}
summary(model5pb)
```

```{r}
model5pc <- lm(average_price ~ type + region + quarter + year + x_large_bags + type:year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pc)
```

```{r}
summary(model5pc)
```

```{r}
model5pd <- lm(average_price ~ type + region + quarter + year + x_large_bags + type:x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pd)
```

```{r}
summary(model5pd)
```


```{r}
model5pe <- lm(average_price ~ type + region + quarter + year + x_large_bags + region:quarter, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pe)
```


```{r}
summary(model5pe)
```

```{r}

model5pf <- lm(average_price ~ type + region + quarter + year + x_large_bags + region:year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pf)
```

```{r}
summary(model5pf)
```

```{r}
model5pg <- lm(average_price ~ type + region + quarter + year + x_large_bags + region:x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pg)
```

```{r}
summary(model5pg)
```

```{r}
model5ph <- lm(average_price ~ type + region + quarter + year + x_large_bags + quarter:year, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5ph)
```

```{r}
summary(model5ph)
```

```{r}
model5pi <- lm(average_price ~ type + region + quarter + year + x_large_bags + quarter:x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pi)
```

```{r}
summary(model5pi)
```

```{r}
model5pj <- lm(average_price ~ type + region + quarter + year + x_large_bags + year:x_large_bags, data = trimmed_avocados)
par(mfrow = c(2, 2))
plot(model5pj)
```

```{r}
summary(model5pj)
```


So it looks like model5pa with the type, region, quarter, year, x_large_bags and type:region is the best, with a moderate gain in multiple-r2 due to the interaction. However, we need to test for the significance of the interaction given the various p-values of the associated coefficients

```{r}
anova(model5, model5pa)
```

Neat, it looks like including the interaction is statistically justified.

Automated approach
```{r}
# we're putting set.seed() in here for reproducibility, but you shouldn't include
# this in production code
set.seed(42)
n_data <- nrow(trimmed_avocados)
test_index <- sample(1:n_data, size = n_data * 0.2)

test  <- slice(trimmed_avocados, test_index)
train <- slice(trimmed_avocados, -test_index)

# sanity check
nrow(test) + nrow(train) == n_data
```

```{r}
nrow(test)

```

```{r}
nrow(train)
```

```{r}
glmulti_fit <- glmulti(
  average_price ~ ., 
  data = train,
  level = 1, # 2 = include pairwise interactions, 1 = main effects only (main effect = no pairwise interactions)
  minsize = 1, # no min size of model
  maxsize = -1, # -1 = no max size of model
  marginality = TRUE, # marginality here means the same as 'strongly hierarchical' interactions, i.e. include pairwise interactions only if both predictors present in the model as main effects.
  method = "h", # try exhaustive search, or could use "g" for genetic algorithm instead
  crit = bic, # criteria for model selection is BIC value (lower is better)
  plotty = FALSE, # don't plot models as function runs
  report = TRUE, # do produce reports as function runs
  confsetsize = 10, # return best 10 solutions
  fitfunction = lm # fit using the `lm` function
)
```

```{r}
summary(glmulti_fit)
```

So the lowest BIC model with main effects is average_price ~ type + year + quarter + total_volume + x_large_bags + region. Let’s have a look at possible extensions to this. We’re going to deliberately try to go to the point where models start to overfit (as tested by the RMSE on the test set), so we’ve seen what this looks like.
```{r}
results <- tibble(
  name = c(), bic = c(), rmse_train = c(), rmse_test = c()
)
```

```{r}
# lowest BIC model with main effects
lowest_bic_model <- lm(average_price ~ type + year + quarter + total_volume + x_large_bags + region, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "lowest bic", 
      bic = bic(lowest_bic_model),
      rmse_train = rmse(lowest_bic_model, train),
      rmse_test = rmse(lowest_bic_model, test)
    )
  )

# try adding in all possible pairs with these main effects
lowest_bic_model_all_pairs <- lm(average_price ~ (type + year + quarter + total_volume + x_large_bags + region)^2, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "lowest bic all pairs", 
      bic = bic(lowest_bic_model_all_pairs),
      rmse_train = rmse(lowest_bic_model_all_pairs, train),
      rmse_test = rmse(lowest_bic_model_all_pairs, test)
    )
  )
```

```{r}
# try a model with all main effects
model_all_mains <- lm(average_price ~ ., data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all mains", 
      bic = bic(model_all_mains),
      rmse_train = rmse(model_all_mains, train),
      rmse_test = rmse(model_all_mains, test)
    )
  )

# try a model with all main effects and all pairs
model_all_pairs <- lm(average_price ~ .^2, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all pairs", 
      bic = bic(model_all_pairs),
      rmse_train = rmse(model_all_pairs, train),
      rmse_test = rmse(model_all_pairs, test)
    )
  )
```

```{r}
# try a model with all main effects, all pairs and one triple (this is getting silly)
model_all_pairs_one_triple <- lm(average_price ~ .^2 + region:type:year, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all pairs one triple",
      bic = bic(model_all_pairs_one_triple),
      rmse_train = rmse(model_all_pairs_one_triple, train),
      rmse_test = rmse(model_all_pairs_one_triple, test)
    )
  )
```

```{r}
# try a model with all main effects, all pairs and multiple triples (more silly)
model_all_pairs_multi_triples <- lm(average_price ~ .^2 + region:type:year + region:type:quarter + region:year:quarter, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all pairs multi triples",
      bic = bic(model_all_pairs_multi_triples),
      rmse_train = rmse(model_all_pairs_multi_triples, train),
      rmse_test = rmse(model_all_pairs_multi_triples, test)
    )
  )
```

```{r}
results <- results %>%
  pivot_longer(cols = bic:rmse_test, names_to = "measure", values_to = "value") %>%
  mutate(
    name = fct_relevel(
      as_factor(name),
      "lowest bic", "all mains", "lowest bic all pairs", "all pairs", "all pairs one triple", "all pairs multi triples"
    )
  )
```


```{r}
results %>%
  filter(measure == "bic") %>%
  ggplot(aes(x = name, y = value)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  labs(
    x = "model",
    y = "bic"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_hline(aes(yintercept = 0))
```

BIC is telling us here that if we took our main effects model with lowest BIC, and added in all possible pairs, this would likely still improve the model for predictive purposes. BIC suggests that this ‘lowest BIC all pairs’ model will offer best predictive performance without overfitting, with all other models being significantly poorer.

```{r}
results %>%
  filter(measure != "bic") %>%
  ggplot(aes(x = name, y = value, fill = measure)) +
  geom_col(position = "dodge", alpha = 0.7) +
  labs(
    x = "model",
    y = "rmse"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


Lowest RMSE in test is obtained for the ‘lowest bic all pairs’ model, and it increases thereafter for the more complex models, which suggests that these models are overfitting the training data.